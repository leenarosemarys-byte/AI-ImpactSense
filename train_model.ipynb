{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_vP45G9hrAU",
        "outputId": "c6148175-bbc2-4303-f3b3-cea51ef84b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TRAINING BASELINE (Logistic Regression) ---\n",
            "Baseline Accuracy: 0.6154\n",
            "\n",
            "--- TRAINING ADVANCED (Random Forest) ---\n",
            "Advanced RF Accuracy: 0.9115\n",
            "\n",
            "Success: Model and Scaler saved in /output folder.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load the data\n",
        "# Assuming your file is named 'earthquake_data.csv'\n",
        "df = pd.read_csv('earthquake_data.csv')\n",
        "\n",
        "# 2. Preprocessing\n",
        "# Map colors to numbers\n",
        "alert_mapping = {'green': 0, 'yellow': 1, 'orange': 2, 'red': 3}\n",
        "df['alert'] = df['alert'].map(alert_mapping)\n",
        "\n",
        "# Keep ONLY your requested 5 features\n",
        "features = ['magnitude', 'depth', 'cdi', 'mmi', 'sig']\n",
        "X = df[features]\n",
        "y = df['alert']\n",
        "\n",
        "# Drop any rows with missing values\n",
        "X = X.dropna()\n",
        "y = y.loc[X.index]\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Scaling (Essential for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"--- TRAINING BASELINE (Logistic Regression) ---\")\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "print(f\"Baseline Accuracy: {accuracy_score(y_test, lr.predict(X_test_scaled)):.4f}\")\n",
        "\n",
        "print(\"\\n--- TRAINING ADVANCED (Random Forest) ---\")\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=3)\n",
        "rf_grid.fit(X_train_scaled, y_train)\n",
        "best_rf = rf_grid.best_estimator_\n",
        "print(f\"Advanced RF Accuracy: {accuracy_score(y_test, best_rf.predict(X_test_scaled)):.4f}\")\n",
        "\n",
        "# 4. SAVE RESOURCES FOR UI\n",
        "if not os.path.exists('output'):\n",
        "    os.makedirs('output')\n",
        "\n",
        "with open('output/final_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_rf, f)\n",
        "\n",
        "with open('output/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "# Save feature names to ensure order is preserved in UI\n",
        "with open('output/feature_names.pkl', 'wb') as f:\n",
        "    pickle.dump(features, f)\n",
        "\n",
        "print(\"\\nSuccess: Model and Scaler saved in /output folder.\")"
      ]
    }
  ]
}